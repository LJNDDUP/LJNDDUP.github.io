---
layout:       post
title:        面经
subtitle:     一个面经
date:         2019-09-01
author:       JAN
header-img:   img/post-bg-alibaba.jpg
catalog:      true
tags:
    - Job interview afterthoughts
---

# 阿里巴巴

## 简历评估面

1. 选一个最具创新性的项目介绍；

2. 介绍计算机视觉经典网络；

3. 如何防止过拟合？简单介绍一下BN；

4. 概率题：给定一条线段，任意裁成三段，能组成三角形的概率；
-- 概率是一个小三角形除以一个大三角形，0.25；许久不做概率题，首先要把抽象问题具体化，数字化，符号化，比如不失一般性，令线段总长为1，其中两条边为$x$和$y$，则第三边为$1-x-y$，一个三角形是$0 < x < 0.5$，$0 < y < 0.5$和$x + y > 0.5$，另一个三角形是$0 < x < 1$，$0 < y < 1$和$0 < x + y < 1$，最终得到答案0.25；  

5. 编程题：从一个无序数组中返回第k大的数；

# 蘑菇街

## 一面

1. 关于深度补全，被问到：因为焦距很近，怎么防止鼻子对到眼睛？（面试官是指焦距很近，视差很小。）差不多忘了当时的回答了，因为我的网络不是先去预测视差再转换成深度的，而是直接预测深度，而且是逐步优化的，防止的方法一方面是从数据集中去学习，另一方面是设计合适的网络架构。

2. 编程题：给定一系列字符串，判断是不是属于同一个类（如果经过有限次交换后能相同，则属于同一个类）。如果是两个字符串，只需要判断排序之后的字符串是否相等即可，一系列字符串则需要增加边界条件的判断。

## 二面

1. 介绍TOF，当时主要介绍了TOF的局限性。

2. 如何防止梯度消失？ResNet为什么能防止梯度消失？

3. 遇到最大的问题，及解决；工作的规划；项目的分工；

# 海康威视

## 答辩

1. LSTM之后，为什么又提出GRU？
-- 当时回答的是GRU的计算量更小一点。GRU的参数更少，因而训练稍快或需要更少的数据来泛化，但如果你有足够的数据，LSTM可能会产生更好的效果；

2. 你认为是哪些改动让你的模型超越了参考论文？哪个起的作用更多一些？
-- 做实验的时候，在同一个毛坯下添加组件，看看是哪个起的作用更多。但当时就像施总说的，可以看下哪个的提高多，从某个方面来说那个组件起的作用相对就大一些。

3. 你这个模型如果我要用，你认为目前最大的阻碍在哪里？
-- 当时的回答是在于输入，用背景建模替代光流的做法不是最完美的做法；

# 虹软

## 一

1. 画一个简单的网络，写一下反向传播的公式；

2. $Ax+b=0$，有没有解的各个情况的分析；

3. 给定$abcd$四个数，找出中间的两个数，比较次数最少；
-- $a < b$，$c < d$，若$b < d$返回$bc$，否则返回$cd$。
三次不够的，应该要四次，$a < b$，$c < d$，取$a$和$c$中的较大者和$b$和$d$中的较小者。

4. 写一下LSTM/GRU的式子；

## 二

根据项目问问题；

## HR

聊天；

# 旷视

## 一

1. bn，bn在预测的时候使用的均值和方差是固定的吗，那均值和方差分别是有偏估计还是无偏估计？

  直接的做法：在模型训练的时候记录下每个batch的均值和方差，待训练完毕后，我们保留了每组mini-batch的均值和方差，使用它们各自的无偏估计作为测试时的均值和方差。$\mu_{test}=E(\mu_{batch})$，$\sigma^2_{test}=\frac{m}{m-1}E(\sigma^2_{batch})$。

  测试快速的做法：在训练的时候，就顺便把测试的均值和方差算出来，即滑动均值和滑动方差。$\mu_{test}=momentum*\mu_{test}+(1-momentum)*\mu_{batch}$，$\sigma^2_{test}=momentum*\sigma^2_{test}+(1-momentum)*\sigma^2_{batch}$ ，是一个一阶指数平滑。

  无偏估计是用样本统计量来估计总体参数时的无偏推断。

2. 计算量参数量感受野，输入是3 * 1920 * 1080，两层卷积参数都是conv3 * 3，stride2，padding1，out_channel64；

3. 判断一个点是否在凸多边形内部，
  struct Point x, y
  struct Polygon p1, p2, ...

