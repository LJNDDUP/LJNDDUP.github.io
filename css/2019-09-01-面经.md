---
layout:       post
title:        面经
subtitle:     一个面经
date:         2019-09-01
author:       JAN
header-img:   img/post-bg-alibaba.jpg
catalog:      true
tags:
    - Job interview afterthoughts
---

# 阿里巴巴

## 简历评估面

1. 选一个最具创新性的项目介绍；

2. 介绍计算机视觉经典网络；

3. 如何防止过拟合？简单介绍一下BN；

4. 概率题：给定一条线段，任意裁成三段，能组成三角形的概率；
-- 概率是一个小三角形除以一个大三角形，0.25；许久不做概率题，首先要把抽象问题具体化，数字化，符号化，比如不失一般性，令线段总长为1，其中两条边为x和y，则第三边为1-x-y，一个三角形是0 < x < 0.5，0 < y < 0.5和x + y > 0.5，另一个三角形是0 < x < 1，0 < y < 1和0 < x + y < 1，最终得到答案0.25；  

5. 编程题：从一个无序数组中返回第k大的数；

# 蘑菇街

## 一面

1. 关于深度补全，被问到：因为焦距很近，怎么防止鼻子对到眼睛？（面试官是指焦距很近，视差很小。）差不多忘了当时的回答了，因为我的网络不是先去预测视差再转换成深度的，而是直接预测深度，而且是逐步优化的，防止的方法一方面是从数据集中去学习，另一方面是设计合适的网络架构。

2. 编程题：给定一系列字符串，判断是不是属于同一个类（如果经过有限次交换后能相同，则属于同一个类）。如果是两个字符串，只需要判断排序之后的字符串是否相等即可，一系列字符串则需要增加边界条件的判断。

## 二面

1. 介绍TOF，当时主要介绍了TOF的局限性。

2. 如何防止梯度消失？ResNet为什么能防止梯度消失？

3. 遇到最大的问题，及解决；工作的规划；项目的分工；

# 海康威视

## 答辩

1. LSTM之后，为什么又提出GRU？
-- 当时回答的是GRU的计算量更小一点。GRU的参数更少，因而训练稍快或需要更少的数据来泛化，但如果你有足够的数据，LSTM可能会产生更好的效果；

2. 你认为是哪些改动让你的模型超越了参考论文？哪个起的作用更多一些？
-- 做实验的时候，在同一个毛坯下添加组件，看看是哪个起的作用更多。但当时就像施总说的，可以看下哪个的提高多，从某个方面来说那个组件起的作用相对就大一些。

3. 你这个模型如果我要用，你认为目前最大的阻碍在哪里？
-- 当时的回答是在于输入，用背景建模替代光流的做法不是最完美的做法；

# 虹软

## 一

1. 画一个简单的网络，写一下反向传播的公式；

2. $Ax+b=0$，有没有解的各个情况的分析；

3. 给定$abcd$四个数，找出中间的两个数，比较次数最少；
-- $a < b$，$c < d$，若$b < d$返回$bc$，否则返回$cd$。

4. 写一下LSTM/GRU的式子；

## 二

根据项目问问题；

## HR

聊天；
