---
layout:       post
title:        损失函数
subtitle:     DL
date:         2019-08-31
author:       JAN
header-img:   img/post-bg-loss.jpg
catalog:      true
mathjax:	  true
tags:
    - DL
---

# 优化方法

梯度下降法是最小化目标函数J的一种方法，利用目标函数关于参数的梯度∂J/∂w的反方向更新参数。

## 批梯度下降法

因为在执行更新的时候，需要在整个数据集上计算所有的梯度，所以BGD的速度较慢，同时，BGD无法处理超出内存容量限制的数据集。BGD也不能在线更新模型，即在运行的过程中，不能添加新的样本。

## 随机梯度下降法

因为数据集中有相似的样本，因此BGD会有冗余；SGD一次只对一个样本进行更新，更新过程会出现波动。一方面波动性使得SGD可以跳动潜在的更好的局部最优，另一方面波动性使得收敛过程变得复杂。当我们缓慢减小学习率，SGD和MBGD具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最优和全局最优。在每一次循环中，打乱训练样本。

## 小批量梯度下降法

减小了参数更新的方差，这样可以得到更加稳定的收敛效果；可以利用深度学习库中高度优化的矩阵计算，通常batchsize设置为50 - 256，MBGD是典型的优化方法，通常也称为SGD。

## Momentum

$$v_t=v_{t-1}+\eta∂w$$

$w=w-v_t$

SGD会在局部极值点附近振荡，从而导致收敛速度变慢。加强当前梯度方向和上一次梯度方向相同的参数，减弱当前梯度方向和上一次梯度方向相反的参数。因此获得更快的收敛速度和减小振荡。

## NAG

$$w=w-\gamma v_{t-1}-\eta∂(w-\gamma v_{t-1})$$

对momentum进行改进，先对参数进行预估，再用预估后的参数计算损失。

## RMSProp

$$s_t=\gamma s_{t-1}+(1-\gamma)(dw)^2$$

$$w=w-\frac{\eta}{\sqrt{s_t+\epsilon}}$$

$dw$指历史平方梯度.

对更新快的参数采用较大的学习率，对更新慢的参数采用较小的学习率。

## ADAM

ADAM与RMSProp不同的是，它计算历史梯度衰减方式采用和Momentum类似的方式。

$$v_t=\beta_1v_{t-1}+(1-\beta_1)∂w$$

$$s_t=\beta_2s_{t-1}+(1-\beta_2)(∂w)^2$$

偏差修正

$$v_t=\frac{v_t}{(1-\beta_1^t)}$$

$$s_t=\frac{s_t}{(1-\beta_2^t)}$$

$$w=w-\eta\frac{v_t}{\sqrt{s_t}+\epsilon}$$

$$\beta_1=0.9, \beta_2=0.999$$

