---
layout:       post
title:        集成学习
subtitle:     ML
date:         2019-09-01
author:       JAN
header-img:   img/post-bg-mistake.jpeg
catalog:      true
mathjax:	  true
tags:
    - ML
---

# 集成学习分为哪几种？他们有何异同？

## Boosting

Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思想是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。Boosting能够提升弱分类器的性能的原因是降低了偏差。

## Bagging

Bagging方法训练基分类器时采用并行的方式，各个基分类器之间无强依赖。Bagging能够提升弱分类器的性能的原因是降低了方差。

典型算法是随机森林，为了让基分类器相互独立，将训练集分为若干子集（当训练样本数量较少时，子集之间可能有重叠），最后通过投票的方式作出最后的集体决策。

# 什么是偏差和方差？

偏差指的是由**所有采样得到的大小为m的训练数据集训练出的所有模型的输出的平均值**和真实模型输出之间的偏差。

方差是指由所有采样得到的大小为m的训练数据集训练出的所有模型输出的方差。

# 常用的基分类器是什么？

最常用的基分类器是决策树，主要有以下三方面的原因。

1. 决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重；

2. 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；

3. 数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性；

除了决策树以外，神经网络模型也适合作为基分类器，主要是由于神经网络模型也比较“不稳定”，而且还可以通过调整神经元数量、连接方式、网络层数、初始权重等方式引入随机性。


